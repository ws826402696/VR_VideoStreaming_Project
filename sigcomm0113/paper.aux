\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{distance}
\citation{PSPNR}
\citation{luminance1}
\citation{PSPNR}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{contrast_masking}
\citation{letter}
\citation{brain}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Performance comparison of Pano v.s. traditional viewpoint-driven video streaming under average throughput of 1Mbps, with bandwidth fluctuation. Each plot indicates a video session. We present 75\% ellipse confidence area.\relax }}{2}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{first_image}{{1}{2}{Performance comparison of Pano v.s. traditional viewpoint-driven video streaming under average throughput of 1Mbps, with bandwidth fluctuation. Each plot indicates a video session. We present 75\% ellipse confidence area.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Motivation}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Current VR streaming solutions}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Monolithic streaming}{2}{subsubsection.2.1.1}}
\citation{viewpoint-driven1}
\citation{viewpoint-driven2}
\citation{viewpoint-driven3}
\citation{viewpoint-driven4}
\citation{viewpoint-driven5}
\citation{viewpoint-driven6}
\citation{Flare}
\citation{viewpoint-driven1}
\citation{VPprediction}
\citation{Flare}
\citation{speed}
\citation{DoF1}
\citation{DoF2}
\citation{adaptation1}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Viewpoint-driven streaming}{3}{subsubsection.2.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}What influences user-perceived quality}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Challenges}{3}{subsection.2.3}}
\citation{PSPNR}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces PSPNR-bandwidth tradeoff in video encoding with different granularity of rate allocation.\relax }}{4}{figure.caption.4}}
\newlabel{optimalencoding}{{2}{4}{PSPNR-bandwidth tradeoff in video encoding with different granularity of rate allocation.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The ratio of video size after tiling and original video size in each tiling granularity and each bitrate level. We notice that fine-grained tiling introduces serious bitrate efficiency problem, especially in low bandwidth situations in which the overall bitrate of each tile is low.\relax }}{4}{figure.caption.5}}
\newlabel{bitrateefficiency}{{3}{4}{The ratio of video size after tiling and original video size in each tiling granularity and each bitrate level. We notice that fine-grained tiling introduces serious bitrate efficiency problem, especially in low bandwidth situations in which the overall bitrate of each tile is low.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Potential improvement}{4}{subsection.2.4}}
\citation{distance}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces PSPNR-bandwidth tradeoff of (1) Viewpoint-driven VR streaming. (2) Perceived quality driven VR streaming. (3) Perceived quality driven VR streaming. (with 3 VR factors)\relax }}{5}{figure.caption.6}}
\newlabel{potential1}{{4}{5}{PSPNR-bandwidth tradeoff of (1) Viewpoint-driven VR streaming. (2) Perceived quality driven VR streaming. (3) Perceived quality driven VR streaming. (with 3 VR factors)\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Key insights and ideas}{5}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Insights}{5}{subsection.3.1}}
\citation{PSPNR}
\citation{JND}
\citation{PSPNR}
\citation{JND}
\citation{luminance1}
\citation{PSPNR}
\citation{PSPNR}
\citation{distance}
\citation{distance}
\newlabel{fig_insight_tilinga}{{\caption@xref {fig_insight_tilinga}{ on input line 27}}{6}{Insights}{figure.caption.7}{}}
\newlabel{fig_insight_tilingb}{{\caption@xref {fig_insight_tilingb}{ on input line 32}}{6}{Insights}{figure.caption.7}{}}
\newlabel{fig_insight_tilingc}{{\caption@xref {fig_insight_tilingc}{ on input line 37}}{6}{Insights}{figure.caption.7}{}}
\newlabel{fig_insight_tilingd}{{\caption@xref {fig_insight_tilingd}{ on input line 42}}{6}{Insights}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces An example of traditional grid-like tiling and object-based tiling.\relax }}{6}{figure.caption.7}}
\newlabel{fig_insight_tiling}{{5}{6}{An example of traditional grid-like tiling and object-based tiling.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Key ideas to solve the challenges}{6}{subsection.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Perceived quality measurement}{6}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Perceived quality measuring by Just-Noticeable Distortion (JND) model}{6}{subsection.4.1}}
\newlabel{f1}{{1}{6}{Perceived quality measuring by Just-Noticeable Distortion (JND) model}{equation.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Building Just-Noticeable-Distortion Model for VR display}{6}{subsection.4.2}}
\citation{distance}
\citation{distance}
\citation{PSPNR}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces JND due to content luminance.\relax }}{7}{figure.caption.8}}
\newlabel{JNDluminance}{{6}{7}{JND due to content luminance.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces JND due to content luminance \& viewpoint-object distance.\relax }}{7}{figure.caption.9}}
\newlabel{JNDlum-dist}{{7}{7}{JND due to content luminance \& viewpoint-object distance.\relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces HMD Parameters\relax }}{7}{table.caption.10}}
\newlabel{table1}{{1}{7}{HMD Parameters\relax }{table.caption.10}{}}
\citation{speed}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Results}{8}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}JND v.s. luminance \& viewpoint moving speed}{8}{subsubsection.4.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces CDF gram of viewpoint moving speed.\relax }}{8}{figure.caption.11}}
\newlabel{CDFspeed}{{8}{8}{CDF gram of viewpoint moving speed.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces JND due to content luminance and viewpoint moving speed.\relax }}{8}{figure.caption.12}}
\newlabel{JNDspeed-lum-track}{{9}{8}{JND due to content luminance and viewpoint moving speed.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces $f_{track}(v)$, the JND coefficient of viewpoint moving speed.\relax }}{8}{figure.caption.13}}
\newlabel{JNDspeed-track}{{10}{8}{$f_{track}(v)$, the JND coefficient of viewpoint moving speed.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces $f_{notrack}(v)$, the JND coefficient of viewpoint moving speed.\relax }}{9}{figure.caption.14}}
\newlabel{JNDspeed-notrack}{{11}{9}{$f_{notrack}(v)$, the JND coefficient of viewpoint moving speed.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces An example of real viewpoint moving pattern and predicted viewpoint moving pattern.\relax }}{9}{figure.caption.15}}
\newlabel{speed_analysis}{{12}{9}{An example of real viewpoint moving pattern and predicted viewpoint moving pattern.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}JND v.s. luminance \& depth of field}{9}{subsubsection.4.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces An example of parallax effect.\relax }}{9}{figure.caption.16}}
\newlabel{v-aconflict}{{13}{9}{An example of parallax effect.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces JND due to focus Depth-of-Field and object Depth-of-Field.\relax }}{9}{figure.caption.17}}
\newlabel{JNDfdof-odof}{{14}{9}{JND due to focus Depth-of-Field and object Depth-of-Field.\relax }{figure.caption.17}{}}
\citation{darkadaptation}
\citation{darkadaptation2}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces JND due to content luminance and object Depth-of-Field.\relax }}{10}{figure.caption.18}}
\newlabel{JNDdof-lum}{{15}{10}{JND due to content luminance and object Depth-of-Field.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces $f_{DoF}(D_o, D_f)$, the JND coefficient of object / focus Depth-of-Field.\relax }}{10}{figure.caption.19}}
\newlabel{JNDdof}{{16}{10}{$f_{DoF}(D_o, D_f)$, the JND coefficient of object / focus Depth-of-Field.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}JND v.s. luminance \& light / dark adaptation}{10}{subsubsection.4.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Viewpoint prediction error v.s. DoF error of $D_f$. We present the average error, along with the upper bound and the lower bound of 75\% cases (around the average case).\relax }}{10}{figure.caption.20}}
\newlabel{depth_analysis}{{17}{10}{Viewpoint prediction error v.s. DoF error of $D_f$. We present the average error, along with the upper bound and the lower bound of 75\% cases (around the average case).\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces JND due to content luminance and light / dark adaptation.\relax }}{10}{figure.caption.21}}
\newlabel{JNDadapt-lum}{{18}{10}{JND due to content luminance and light / dark adaptation.\relax }{figure.caption.21}{}}
\citation{PSPNR}
\citation{luminance1}
\citation{PSPNR}
\citation{distance}
\citation{luminance1}
\citation{PSPNR}
\citation{distance}
\citation{PSPNR}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces $f_{adapt}(\Delta e)$, the JND coefficient of light / dark adaptation.\relax }}{11}{figure.caption.22}}
\newlabel{JNDadapt}{{19}{11}{$f_{adapt}(\Delta e)$, the JND coefficient of light / dark adaptation.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Put it together}{11}{subsubsection.4.3.4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces symbol and definition used for JND computation\relax }}{11}{table.caption.23}}
\newlabel{table2}{{2}{11}{symbol and definition used for JND computation\relax }{table.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Object-based tiling scheme}{11}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Object Detection based on Quality-Bitrate Efficiency}{11}{subsection.5.1}}
\newlabel{QBE}{{9}{11}{Object Detection based on Quality-Bitrate Efficiency}{equation.5.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Tiling video by objects}{12}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces The PSPNR-bandwidth tradeoff of proposed Object-based Tiling scheme and traditional grid-like tiling scheme (3*6, 6*12 and 12*24).\relax }}{12}{figure.caption.24}}
\newlabel{tiling}{{20}{12}{The PSPNR-bandwidth tradeoff of proposed Object-based Tiling scheme and traditional grid-like tiling scheme (3*6, 6*12 and 12*24).\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Comparison of object-based tiling and grid-like tiling}{12}{subsection.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Client-side PSPNR computation and optimization}{12}{section.6}}
\citation{BOLA}
\citation{MPC}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Client-side PSPNR computation}{13}{subsection.6.1}}
\newlabel{CJND}{{13}{13}{Client-side PSPNR computation}{equation.6.13}{}}
\newlabel{BJND}{{14}{13}{Client-side PSPNR computation}{equation.6.14}{}}
\newlabel{f1}{{15}{13}{Client-side PSPNR computation}{equation.6.15}{}}
\newlabel{apprxPMSE}{{16}{13}{Client-side PSPNR computation}{equation.6.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces CDF of PSPNR approximation accuracy.\relax }}{13}{figure.caption.25}}
\newlabel{PSPNR_computation}{{21}{13}{CDF of PSPNR approximation accuracy.\relax }{figure.caption.25}{}}
\newlabel{apprx}{{17}{13}{Client-side PSPNR computation}{equation.6.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}PSPNR optimization}{13}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Level 1: Allocating bitrate of each temporal segment}{13}{subsubsection.6.2.1}}
\citation{BOLA}
\citation{github}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}Level 2: Allocating bitrate of each spatial tiles}{14}{subsubsection.6.2.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces  PSPNR driven Rate Allocation Algorithm.\relax }}{14}{algorithm.1}}
\newlabel{alg:Framwork}{{1}{14}{PSPNR driven Rate Allocation Algorithm.\relax }{algorithm.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Workflow of PQVRS.\relax }}{14}{figure.caption.26}}
\newlabel{implementation}{{22}{14}{Workflow of PQVRS.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Implementation}{14}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Server-Side}{14}{subsection.7.1}}
\citation{yolo}
\citation{Flare}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Client-Side}{15}{subsection.7.2}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Performance Evaluation}{15}{section.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Network topology.\relax }}{15}{figure.caption.27}}
\newlabel{network}{{23}{15}{Network topology.\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Evaluation Setup}{15}{subsection.8.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces PSPNR-bandwidth tradeoff of 5 methods: Viewpoint-driven, PQVRS, PQVRS+, PQVRS++, theoretical performance.\relax }}{16}{figure.caption.28}}
\newlabel{practical_imp}{{24}{16}{PSPNR-bandwidth tradeoff of 5 methods: Viewpoint-driven, PQVRS, PQVRS+, PQVRS++, theoretical performance.\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Performance under network with different bandwidth stability}{16}{subsection.8.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}Performance comparison under fixed bandwidth}{16}{subsubsection.8.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2}Performance comparison under real-world bandwidth}{16}{subsubsection.8.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces PSPNR of 5 methods: Viewpoint-driven, PQVRS, PQVRS+, PQVRS++, theoretical performance.\relax }}{16}{figure.caption.29}}
\newlabel{practical_PSPNR}{{25}{16}{PSPNR of 5 methods: Viewpoint-driven, PQVRS, PQVRS+, PQVRS++, theoretical performance.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Stalling of 5 methods: Viewpoint-driven, PQVRS, PQVRS+, PQVRS++, theoretical performance.\relax }}{16}{figure.caption.30}}
\newlabel{practical_stall}{{26}{16}{Stalling of 5 methods: Viewpoint-driven, PQVRS, PQVRS+, PQVRS++, theoretical performance.\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Performance robustness under random user behavior}{16}{subsection.8.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces PSPNR improvement of PQVRS, PQVRS+, PQVRS++ and theoretical cases on random viewpoint traces, compared with viewpoint-driven VR streaming.\relax }}{17}{figure.caption.31}}
\newlabel{random_improvement}{{27}{17}{PSPNR improvement of PQVRS, PQVRS+, PQVRS++ and theoretical cases on random viewpoint traces, compared with viewpoint-driven VR streaming.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Real-world user rating}{17}{subsection.8.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces User rating rules.\relax }}{17}{figure.caption.32}}
\newlabel{rating_rules}{{28}{17}{User rating rules.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces User rating results.\relax }}{17}{figure.caption.33}}
\newlabel{rating_res}{{29}{17}{User rating results.\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Analysis of VR streaming workload}{17}{subsection.8.5}}
\citation{distance}
\citation{PSPNR}
\citation{brain}
\citation{viewpoint-driven1}
\citation{viewpoint-driven2}
\citation{viewpoint-driven3}
\citation{viewpoint-driven4}
\citation{viewpoint-driven5}
\citation{viewpoint-driven6}
\citation{Flare}
\bibstyle{ACM-Reference-Format}
\bibdata{reference}
\bibcite{github}{{1}{[n. d.]}{{git}}{{??}}}
\bibcite{letter}{{2}{1974}{{Anstis}}{{Anstis}}}
\bibcite{viewpoint-driven1}{{3}{2017}{{Bao et~al\unhbox \voidb@x \hbox {.}}}{{Bao, Wu, Zhang, Ramli, and Liu}}}
\bibcite{DoF1}{{4}{2015}{{Carnegie and Rhee}}{{Carnegie and Rhee}}}
\bibcite{distance}{{5}{2009}{{Chen and Guillemot}}{{Chen and Guillemot}}}
\bibcite{JND}{{6}{1996}{{Chou and Chen}}{{Chou and Chen}}}
\bibcite{PSPNR}{{7}{1995}{{Chou and Li}}{{Chou and Li}}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Client-side CPU workload comparison between Flare, PQVRS and PQVRS++.\relax }}{18}{figure.caption.34}}
\newlabel{CPUclient}{{30}{18}{Client-side CPU workload comparison between Flare, PQVRS and PQVRS++.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Server-side CPU workload comparison between Flare, PQVRS and PQVRS++.\relax }}{18}{figure.caption.35}}
\newlabel{CPUserver}{{31}{18}{Server-side CPU workload comparison between Flare, PQVRS and PQVRS++.\relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Related Works}{18}{section.9}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Conclusion}{18}{section.10}}
\@writefile{toc}{\contentsline {section}{References}{18}{section*.37}}
\bibcite{viewpoint-driven2}{{8}{2016}{{Gaddam et~al\unhbox \voidb@x \hbox {.}}}{{Gaddam, Riegler, Eg, Halvorsen, and Griwodz}}}
\bibcite{viewpoint-driven3}{{9}{2017}{{Graf et~al\unhbox \voidb@x \hbox {.}}}{{Graf, Timmerer, and Mueller}}}
\bibcite{DoF2}{{10}{2008}{{Hoffman et~al\unhbox \voidb@x \hbox {.}}}{{Hoffman, Girshick, Akeley, and Banks}}}
\bibcite{viewpoint-driven4}{{11}{2016}{{Hosseini and Swaminathan}}{{Hosseini and Swaminathan}}}
\bibcite{contrast_masking}{{12}{1980}{{Legge and Foley}}{{Legge and Foley}}}
\bibcite{darkadaptation}{{13}{1974a}{{Normann and Werblin}}{{Normann and Werblin}}}
\bibcite{adaptation1}{{14}{1974b}{{Normann and Werblin}}{{Normann and Werblin}}}
\bibcite{viewpoint-driven5}{{15}{2017}{{Petrangeli et~al\unhbox \voidb@x \hbox {.}}}{{Petrangeli, Swaminathan, Hosseini, and De~Turck}}}
\bibcite{darkadaptation2}{{16}{1975}{{Pugh}}{{Pugh}}}
\bibcite{Flare}{{17}{2018}{{Qian et~al\unhbox \voidb@x \hbox {.}}}{{Qian, Han, Xiao, and Gopalakrishnan}}}
\bibcite{VPprediction}{{18}{2016}{{Qian et~al\unhbox \voidb@x \hbox {.}}}{{Qian, Ji, Han, and Gopalakrishnan}}}
\bibcite{yolo}{{19}{2017}{{Redmon and Farhadi}}{{Redmon and Farhadi}}}
\bibcite{luminance1}{{20}{1989}{{Safranek and Johnston}}{{Safranek and Johnston}}}
\bibcite{BOLA}{{21}{2016}{{Spiteri et~al\unhbox \voidb@x \hbox {.}}}{{Spiteri, Urgaonkar, and Sitaraman}}}
\bibcite{speed}{{22}{1995}{{Westerink and Teunissen}}{{Westerink and Teunissen}}}
\bibcite{MPC}{{23}{2015}{{Yin et~al\unhbox \voidb@x \hbox {.}}}{{Yin, Jindal, Sekar, and Sinopoli}}}
\bibcite{viewpoint-driven6}{{24}{2016}{{Zare et~al\unhbox \voidb@x \hbox {.}}}{{Zare, Aminlou, Hannuksela, and Gabbouj}}}
\bibcite{brain}{{25}{2011}{{Zhao et~al\unhbox \voidb@x \hbox {.}}}{{Zhao, Yu, Chen, and Zhu}}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{9.29999pt}
\newlabel{tocindent2}{11.49998pt}
\newlabel{tocindent3}{20.22pt}
\newlabel{TotPages}{{19}{19}{}{page.19}{}}
