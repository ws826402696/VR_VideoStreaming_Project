\documentclass[sigconf]{acmart}

\usepackage[english]{babel}
\usepackage{blindtext}
\usepackage{lipsum}


% Copyright
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference info
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

\usepackage{xspace}

\settopmatter{printacmref=false, printccs=false, printfolios=true}

% DOI
\acmDOI{}

% ISBN
\acmISBN{}

%Conference
%\acmConference[Submitted for review to SIGCOMM]{}
%\acmYear{2018}
%\copyrightyear{}

%% {} with no args suppresses printing of the price
\acmPrice{}


\newcommand{\jc}[1]{{\color{blue}{[JC: #1]}}}

\newcommand{\fillme}{{\bf XXX}~}
\newcommand{\name}{{Pano}\xspace}
\newcommand{\vrjnd}{{360JND}\xspace}

\newcounter{packednmbr}
\newenvironment{packedenumerate}{\begin{list}{\thepackednmbr.}{\usecounter{packednmbr}\setlength{\itemsep}{0.5pt}\addtolength{\labelwidth}{-4pt}\setlength{\leftmargin}{2ex}\setlength{\listparindent}{\parindent}\setlength{\parsep}{1pt}\setlength{\topsep}{0pt}}}{\end{list}}
\newenvironment{packeditemize}{\begin{list}{$\bullet$}{\setlength{\itemsep}{0.5pt}\addtolength{\labelwidth}{-4pt}\setlength{\leftmargin}{2ex}\setlength{\listparindent}{\parindent}\setlength{\parsep}{1pt}\setlength{\topsep}{0pt}}}{\end{list}}
\newenvironment{packedpackeditemize}{\begin{list}{$\bullet$}{\setlength{\itemsep}{0.5pt}\addtolength{\labelwidth}{-4pt}\setlength{\leftmargin}{\labelwidth}\setlength{\listparindent}{\parindent}\setlength{\parsep}{1pt}\setlength{\topsep}{0pt}}}{\end{list}}
\newenvironment{packedtrivlist}{\begin{list}{\setlength{\itemsep}{0.2pt}\addtolength{\labelwidth}{-4pt}\setlength{\leftmargin}{\labelwidth}\setlength{\listparindent}{\parindent}\setlength{\parsep}{1pt}\setlength{\topsep}{0pt}}}{\end{list}}



\newcommand{\tightcaption}[1]{\vspace{-0.22cm}\caption{{\normalfont{\textit{{#1}}}}}\vspace{-0.cm}}
\newcommand{\tightsection}[1]{\vspace{-0.25cm}\section{#1}\vspace{-0.08cm}}
\newcommand{\tightsubsection}[1]{\vspace{-0.35cm}\subsection{#1}\vspace{-0.08cm}}
\newcommand{\tightsubsubsection}[1]{\vspace{-0.01in}\subsubsection{#1}\vspace{-0.01cm}}

\newcommand{\eg}{{\it e.g.,}\xspace}
\newcommand{\ie}{{\it i.e.,}\xspace}
\newcommand{\etal}{{\it et.~al}\xspace}
\newcommand{\bigO}{\mathrm{O}}

\newcommand{\myparashort}[1]{\vspace{0.02cm}\noindent{\bf {#1}}~}
\newcommand{\mypara}[1]{\vspace{0.02cm}\noindent{\bf {#1}:}~}
\newcommand{\myparatight}[1]{\vspace{0.03cm}\noindent{\bf {#1}:}~}
\newcommand{\myparaq}[1]{\smallskip\noindent{\bf {#1}?}~}
\newcommand{\myparaittight}[1]{\smallskip\noindent{\emph {#1}:}~}
\newcommand{\question}[1]{\smallskip\noindent{\emph{Q:~#1}}\smallskip}
\newcommand{\myparaqtight}[1]{\smallskip\noindent{\bf {#1}}~}

%\makeatletter
% \def\@textbottom{\vskip \z@ \@plus 1pt}
% \let\@texttop\relax
%\makeatother

\begin{document}
\title{\name: Maximizing Human-Perceived Quality for 360-Degree Video Streaming}

%\titlenote{Produces the permission block, and copyright information}
%\subtitle{Extended Abstract}

\author{Paper \# XXX, XXX pages}
% \author{Firstname Lastname}
% \authornote{Note}
% \orcid{1234-5678-9012}
% \affiliation{%
%   \institution{Affiliation}
%   \streetaddress{Address}
%   \city{City} 
%   \state{State} 
%   \postcode{Zipcode}
% }
% \email{email@domain.com}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{X.et al.}

\begin{abstract}

\end{abstract}

\maketitle

\section{Introduction}


A desirable video streaming protocol should maximize user-perceived video quality under dynamic network conditions.
Thus, the fundamental tension in any video streaming protocol is between the encoding efficiency and the agility of adaptation.
On one hand, if we know {\em a priori} that the bandwidth will be stable when a video segment is streamed, we can maximize the video quality by eliminating the spatio-temporal redundancies in pixels with high encoding rate.
On the other hand, if the video is encoded too aggressively, it would be hard to adapt the quality level fast enough under bandwidth fluctuations.
%A case-in-point of this tradeoff is DASH~\cite{??,??,??}, which recently took hold as the mainstream video protocol. 
%It balances the encoding efficiency and adaptation by splitting the video to fixed-length chunks (4-10 seconds) each encoded in discrete quality levels, allowing it to switch quality at chunk boundaries.


The recent surge of 360-degree videos poses additional challenges to this tension between encoding efficiency and adaptation agility. 
As almost all major content providers have rolled out 360-degree videos (YouTube~\cite{??}, Netflix~\cite{??}, Vimeo~\cite{??}, Amazon Prime~\cite{??}, Hulu~\cite{??}, iQIYI~\cite{??}, YouKu~\cite{??}, etc), 
it has become clear that the 360-degree videos are more challenging than traditional video streaming for two reasons.
First, 360-degree videos must stream much more data (both in 360-degree and in higher resolution) without any interruption in order to create the immersive experience.
Second, although the video streamer can send only the area directly facing the viewer (usually \fillme\% of the total area), doing so forces the encoder to use chunks with much shorter durations (typically, one second) to adapt to the real-time viewport movements, which significantly reduces the encoding efficiency.


In this paper, we argue that there are many unexploited opportunities to optimize 360-degree video quality through a {\em deeper understanding of the user-perceived video quality}.
The assumption behind many existing 360-degree video protocols is that the perceived quality of a spatial region in the video depends on how far the region is to the viewport center.
However, we found that the perceived quality of a 360-degree video can be heavily influenced by several other factors---the {\em velocity} of the head movement, the {\em relative depth} of the viewed objects, and the {\em relative brightness} of the objects compared to those the user have just watched. 
%That is the viewport position and video quality, we found that a 360-degree video at least three reasons: the velocity of head movement, how the relative depth of the viewed objects, as well as the brightness of the objects compared to those the user have just watched. 
In our user study, for instance, when shown the same video at different quality levels, most viewers can perceive quality difference between videos when their viewport is static, but cannot tell any visible difference when the users move their viewport (\eg shaking head or browsing the landscape).
Similarly, how sensitive the viewer is to a given quality difference depends greatly on how further/closer the object is or how much brighter/darker compared to objects that the viewer just saw.
Note that unlike in traditional videos, these factors are driven by ``user behaviors'', and thus unique to 360-degree videos (see \S\ref{sec:motivation} for more discussions).


The findings that humans have only bounded sensitivity to quality degradation can have great implications for 360-degree video streaming. 
In theory, an ideal video streaming protocol can maximize perceived quality under limited available bandwidth, if it increases quality always by a perceivable amount and saves bandwidth by reducing the quality by just an imperceivable amount. 
Based on our user study over \fillme videos and \fillme viewers, we found that one can save \fillme\% bandwidth without any drop in perceived quality! (More discussions in \S\ref{sec:motivation:potential})


The root of these opportunities is the fact that people have a limited span of attention.
Consider when a viewer moves her head quickly, it does increase the viewport-covered area which has to be streamed and rendered, but at the same time, the viewer's attention per-pixel (\ie sensitivity to quality degradation) also drops due to the limited span of attention.
The similar idea that has been used by video encoders to compress videos via exploiting the ``Just Noticeable Difference'' (JND)~\cite{??,??,??}---the minimal visual difference perceivable by humans, \eg viewers generally have larger JND when watching a more complex scene. 
However, conventional JND only models the content-specific factors (since it assumes a static viewport), as opposed to the factors driven by changes . 

This paper presents {\em \name}, an encoding and streaming system that optimizes the user-perceived quality of 360-degree videos. 
\name makes three contributions.

\vspace{0.2cm}
{\em First, \name is based on a new JND model which incorporates the factors driven by 360-degree video viewer actions.}
We did a user study to model the impact of four viewer action-driven factors---movement velocity, relative depth-of-view, relative darkness, and distance-to-viewport---on viewer's sensitivity to quality degradation.
The resulting JND model, referred to as {\em \vrjnd}, can predict, for any given video and viewport trajectory, the amount of quality changes  that are likely be perceivable by the viewer.

An interesting empirical finding is that the new viewer-driven factors are largely independent to each other.
This vastly simplifies the modeling of impact of multiple factors on JND, which would otherwise have to explore the space of a exponential number of combinations of multiple factors.
Their likely independence might explained by \jc{what's the intuition here?}


\vspace{0.2cm}
{\em Second, \name proposes a novel chunking mechanism to fully utilize the new JND model.}
To best utilize the proposed \vrjnd,  a video should ideally be segmented spatially into tiles whose boundaries are aligned with those of the values in the \vrjnd, which typically are the boundaries of objects.
Unfortunately, we found the existing equal-sized square tiling (\eg 12$\times$6) often is far from ideal---the tiles can be too coarse- or too fine-grained. 
In fairness, the square tiling is designed to serve only one purpose of differentiating regions closer to the viewport center from the rest.

\name takes a different path. 
\name splits the 360-degree video into square tiles of different sizes, each of which is one-second in duration and roughly matches the objects in the video. Each tile is then encoded in multiple levels of quantization parameter (QPs), like in existing tiling schemes.
In some sense, this is a compromise between the more radical region-of-interest encoding (in which each object can be encoded differently) and the square tiling which simplifies the rendering of multiple tiles each with different QP levels.



\vspace{0.2cm}
{\em Finally, \name uses a DASH-compatible streaming protocol that adapts to dynamic user actions and network conditions.}
A key challenge of \vrjnd is that any \vrjnd-based quality adaptation needs the access to the video data on the server (\ie cannot be done by the client itself), so it is incompatible with the mainstream DASH protocols which stream videos from stateless HTTP web servers. 
%to determine the perceived quality of a chunk require some computation based on the video data on the server side, so in theory, it is incompatible with the mainstream DASH protocols which stream videos from stateless HTTP web servers.
This is not a problem in non-360-degree videos or existing 360-degree video protocols, because calculating the perceived quality of a chunk only depends on the available bandwidth and user viewport, both are locally accessible on the client side.
However, the perceived quality in \vrjnd depends on the video content and user actions (\ie viewport trajectory and its projection), so the client cannot estimate the user-perceived quality without having the video content in the first place. 
%One approach to this conundrum is to let the video client upload the user action information to the server which then makes adaptation decisions, but this goes against DASH's HTTP-based architecture.

Instead, in this work, we have taken a pragmatic stance to work within the constraints that have spurred the growth of video traffic---streaming videos from web servers over HTTP, like in DASH.
The basic idea is that it is possible to pre-compute the best quality levels for only a few carefully picked possible values of the user action information (viewport trajectory and its projection), and send these estimates as part of the DASH metafile to the client at the beginning of a video session. 
Thus, during playtime the client will be able to estimate the quality level by finding a ``nearest neighbor'' in the user inputs that have been pre-computed. 
The intuition behind pre-computing only a few user-action inputs is an empirical observation that in \vrjnd the impact of the user-action input is non-linear; \eg when the viewport movement speed is over some threshold, its impact on \vrjnd and the QP adaptation changes only marginally (probably because the viewer will be very insensitive).


\vspace{0.2cm}
We implemented a prototype of \name.
\jc{on what platform}
We ran a pilot study on one content provider with \fillme sessions. \jc{how many viewers? what types of videos?}
Our experiments show that \name can save 45\% bandwidth compared with state-of-art 360-video streaming solutions, without decrease of perceived quality.
\jc{add a line to describe the final user-rating based end-to-end experiments}



\bibliographystyle{ACM-Reference-Format}
\bibliography{reference}

\end{document}
